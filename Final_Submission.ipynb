{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Style Visualizer Code\n",
        "The presented code works best as described on Google Colab, to make it work on local environments, we might need to setup NVIDIA CUDA drivers sepparately and add additional checks to make sure that our code recognizes the NVIDIA GPU's."
      ],
      "metadata": {
        "id": "QfSFznivRxoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "First install all the dependencies by running the cell below and it might ask you to restart the session, so please do so. After we have restarted the session, leaving the cell installing packages, run the remaining code cells to get the desired output."
      ],
      "metadata": {
        "id": "1sk0ZZ1wR7j-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies with version control\n",
        "!pip install tensorflow==2.12.0 tensorflow-hub gradio diffusers transformers accelerate nltk sentence-transformers"
      ],
      "metadata": {
        "id": "DfKJkzAIR4_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the NLP aspect of the code working we needed proper implementation of NLTK data, and ensure that all of them downloads properly because while implementing the NLP aspect, many a times we got some error regarding missing package due to which the emotion recognition was unable to work."
      ],
      "metadata": {
        "id": "fFg7RMENSWlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure NLTK properly\n",
        "import nltk\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
        "import os\n",
        "nltk_data_path = '/content/nltk_data'\n",
        "os.makedirs(nltk_data_path, exist_ok=True)\n",
        "nltk.download('punkt', download_dir=nltk_data_path)\n",
        "nltk.download('averaged_perceptron_tagger', download_dir=nltk_data_path)\n",
        "nltk.download('wordnet', download_dir=nltk_data_path)\n",
        "nltk.download('omw-1.4', download_dir=nltk_data_path)\n",
        "nltk.data.path.append(nltk_data_path)\n",
        "\n",
        "# Create a custom sentence tokenizer that doesn't rely on punkt_tab\n",
        "def custom_sent_tokenize(text):\n",
        "    \"\"\"Custom sentence tokenizer that uses PunktSentenceTokenizer directly\"\"\"\n",
        "    # Initialize the tokenizer without loading from punkt_tab\n",
        "    tokenizer = PunktSentenceTokenizer()\n",
        "    return tokenizer.tokenize(text)\n"
      ],
      "metadata": {
        "id": "w6SJ2sMWBzGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import re\n",
        "import functools\n",
        "\n",
        "# Import new NLP components\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "z66at_GdB0ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n"
      ],
      "metadata": {
        "id": "_4o26IPYB2Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load models\n",
        "@functools.lru_cache(maxsize=None)\n",
        "def load_models():\n",
        "    # Stable Diffusion\n",
        "    sd_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-2-1\",\n",
        "        torch_dtype=torch.float16,\n",
        "        safety_checker=None\n",
        "    ).to(device)\n",
        "\n",
        "    # Style Transfer\n",
        "    hub_module = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
        "    stylize_fn = hub_module.signatures['serving_default']\n",
        "\n",
        "    # NLP Models for Enhanced Style Detection\n",
        "    style_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    emotion_classifier = pipeline(\n",
        "        \"text-classification\",\n",
        "        model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
        "        return_all_scores=True\n",
        "    )\n",
        "\n",
        "    return sd_pipe, stylize_fn, style_encoder, emotion_classifier\n",
        "\n",
        "sd_pipe, stylize_fn, style_encoder, emotion_classifier = load_models()\n"
      ],
      "metadata": {
        "id": "ATs6bS2vB4LM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Style configuration - keep the existing mapping but enhance with more metadata\n",
        "STYLE_MAPPING = {\n",
        "    'dreamy': {\n",
        "        'url': 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg',\n",
        "        'keywords': ['peaceful', 'golden', 'serene'],\n",
        "        'description': \"Serene, ethereal scenes with soft lighting\"\n",
        "    },\n",
        "    'dark': {\n",
        "        'url': 'https://upload.wikimedia.org/wikipedia/commons/c/c5/Edvard_Munch%2C_1893%2C_The_Scream%2C_oil%2C_tempera_and_pastel_on_cardboard%2C_91_x_73_cm%2C_National_Gallery_of_Norway.jpg',\n",
        "        'keywords': ['dark', 'stormy', 'shadow'],\n",
        "        'description': \"Dramatic, ominous scenes with shadows\"\n",
        "    },\n",
        "    'vibrant': {\n",
        "        'url': 'https://upload.wikimedia.org/wikipedia/commons/b/b4/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg',\n",
        "        'keywords': ['bright', 'colorful', 'lively'],\n",
        "        'description': \"Colorful, energetic scenes with vivid details\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Emotion to style mapping\n",
        "EMOTION_PRIORITY = {\n",
        "    'neutral': 'dreamy',\n",
        "    'fear': 'dark',\n",
        "    'sadness': 'dark',\n",
        "    'joy': 'vibrant',\n",
        "    'surprise': 'dreamy',\n",
        "    'anger': 'dark',\n",
        "    'disgust': 'dark'\n",
        "}\n"
      ],
      "metadata": {
        "id": "RY58cYb6B5rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@functools.lru_cache(maxsize=None)\n",
        "def load_style_image(style_url):\n",
        "    \"\"\"Load and preprocess style image to 256x256\"\"\"\n",
        "    image_path = tf.keras.utils.get_file(os.path.basename(style_url)[-128:], style_url)\n",
        "    img = tf.io.decode_image(tf.io.read_file(image_path), channels=3, dtype=tf.float32)[tf.newaxis, ...]\n",
        "    img = tf.image.resize(img, (256, 256))\n",
        "    return tf.nn.avg_pool(img, ksize=[3,3], strides=[1,1], padding='SAME')\n"
      ],
      "metadata": {
        "id": "tpb7C0XhB7bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_mood_enhanced(text):\n",
        "    \"\"\"Enhanced mood detection using both keyword analysis and ML models\"\"\"\n",
        "    # Legacy keyword-based analysis\n",
        "    text_lower = text.lower()\n",
        "    keyword_scores = {mood: sum(1 for kw in data['keywords'] if kw in text_lower)\n",
        "                     for mood, data in STYLE_MAPPING.items()}\n",
        "\n",
        "    # Emotion-based analysis\n",
        "    try:\n",
        "        emotion_results = emotion_classifier(text)[0]\n",
        "        dominant_emotion = max(emotion_results, key=lambda x: x['score'])\n",
        "        emotion_style = EMOTION_PRIORITY.get(dominant_emotion['label'], 'vibrant')\n",
        "        emotion_confidence = dominant_emotion['score']\n",
        "    except Exception as e:\n",
        "        print(f\"Emotion analysis error: {e}\")\n",
        "        emotion_style = 'vibrant'\n",
        "        emotion_confidence = 0.0\n",
        "\n",
        "    # Semantic similarity analysis\n",
        "    try:\n",
        "        text_embedding = style_encoder.encode(text)\n",
        "        style_embeddings = {\n",
        "            style: style_encoder.encode(data['description'])\n",
        "            for style, data in STYLE_MAPPING.items()\n",
        "        }\n",
        "        similarities = {\n",
        "            style: float(np.dot(text_embedding, style_emb))\n",
        "            for style, style_emb in style_embeddings.items()\n",
        "        }\n",
        "        semantic_style = max(similarities, key=similarities.get)\n",
        "        semantic_confidence = similarities[semantic_style]\n",
        "    except Exception as e:\n",
        "        print(f\"Semantic analysis error: {e}\")\n",
        "        semantic_style = 'vibrant'\n",
        "        semantic_confidence = 0.0\n",
        "\n",
        "    # Combine analyses with weights - prioritize emotion and semantic similarity over keywords\n",
        "    combined_scores = {}\n",
        "    for style in STYLE_MAPPING.keys():\n",
        "        combined_scores[style] = (\n",
        "            (0.2 * keyword_scores.get(style, 0)) +\n",
        "            (0.4 * (1.0 if style == emotion_style else 0.0) * emotion_confidence) +\n",
        "            (0.4 * similarities.get(style, 0.0))\n",
        "        )\n",
        "\n",
        "    best_mood = max(combined_scores, key=combined_scores.get)\n",
        "\n",
        "    # Extract matched keywords for debugging\n",
        "    matched_keywords = [kw for kw in STYLE_MAPPING[best_mood]['keywords'] if kw in text_lower]\n",
        "\n",
        "    # Return additional analysis details for debugging\n",
        "    analysis_details = {\n",
        "        'keyword_match': keyword_scores,\n",
        "        'emotion_analysis': {'style': emotion_style, 'confidence': emotion_confidence},\n",
        "        'semantic_analysis': {'style': semantic_style, 'confidence': semantic_confidence},\n",
        "        'combined_scores': combined_scores\n",
        "    }\n",
        "\n",
        "    return best_mood, matched_keywords, analysis_details\n"
      ],
      "metadata": {
        "id": "O7SvlqyVB88l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_content_image(prompt):\n",
        "    \"\"\"Generate base image using Stable Diffusion with detailed prompts\"\"\"\n",
        "    try:\n",
        "        detailed_prompt = f\"{prompt}, highly detailed, realistic, cinematic lighting\"\n",
        "        with torch.autocast(device.type):\n",
        "            result = sd_pipe(\n",
        "                detailed_prompt,\n",
        "                guidance_scale=7.5,\n",
        "                height=512,\n",
        "                width=512,\n",
        "                num_inference_steps=50\n",
        "            )\n",
        "        return result.images[0]\n",
        "    except Exception as e:\n",
        "        print(f\"Image generation failed: {e}\")\n",
        "        return Image.new('RGB', (512, 512), color='gray')\n"
      ],
      "metadata": {
        "id": "KV1NRzrJCAbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== IMAGE PROCESSING FUNCTIONS ==========\n",
        "def process_scene(scene_text, style_image, mood, keywords, analysis_details=None):\n",
        "    \"\"\"Process individual scene through full pipeline with debugging details\"\"\"\n",
        "    # Generate content image\n",
        "    content_image = generate_content_image(scene_text)\n",
        "\n",
        "    # Convert content image to TensorFlow tensor\n",
        "    content_tensor = tf.image.resize(\n",
        "        tf.keras.preprocessing.image.img_to_array(content_image)[tf.newaxis, ...] / 255.0,\n",
        "        (256, 256)\n",
        "    )\n",
        "\n",
        "    # Apply Neural Style Transfer\n",
        "    outputs = stylize_fn(\n",
        "        placeholder=content_tensor,\n",
        "        placeholder_1=style_image\n",
        "    )\n",
        "\n",
        "    # Convert styled output to PIL Image\n",
        "    styled_array = (np.clip(outputs['output_0'].numpy()[0], 0, 1) * 255).astype(np.uint8)\n",
        "    styled_image = Image.fromarray(styled_array)\n",
        "\n",
        "    # Return all debugging details\n",
        "    return {\n",
        "        \"source_image\": content_image,\n",
        "        \"styled_image\": styled_image,\n",
        "        \"style_applied\": mood,\n",
        "        \"keywords_used\": keywords,\n",
        "        \"scene_text\": scene_text[:50]+\"...\",  # Truncated scene text for clarity\n",
        "        \"analysis_details\": analysis_details  # Include the full analysis details\n",
        "    }\n"
      ],
      "metadata": {
        "id": "W0mDg4cPCBrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_story(story_text):\n",
        "    \"\"\"Main processing pipeline with enhanced NLP analysis\"\"\"\n",
        "    # Use our custom sentence tokenizer that doesn't rely on punkt_tab\n",
        "    scenes = custom_sent_tokenize(story_text)\n",
        "    outputs = []\n",
        "\n",
        "    for scene in scenes:\n",
        "        if len(scene.strip()) < 5:\n",
        "            continue\n",
        "\n",
        "        # Analyze mood with enhanced NLP approach\n",
        "        mood, keywords, analysis_details = analyze_mood_enhanced(scene)\n",
        "\n",
        "        # Load style image based on mood\n",
        "        style_url = STYLE_MAPPING[mood]['url']\n",
        "        style_image = load_style_image(style_url)\n",
        "\n",
        "        # Process scene and collect all details\n",
        "        scene_details = process_scene(scene, style_image, mood, keywords, analysis_details)\n",
        "        outputs.append(scene_details)\n",
        "\n",
        "    return outputs\n"
      ],
      "metadata": {
        "id": "XOQxdL4bCCN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio Interface\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as app:\n",
        "    gr.Markdown(\"# ðŸ“– Dynamic Story Visualizer with Enhanced NLP\")\n",
        "\n",
        "    with gr.Row():\n",
        "        story_input = gr.Textbox(label=\"Your Story\", placeholder=\"Once upon a time...\", lines=5)\n",
        "        generate_btn = gr.Button(\"Generate Visual Story ðŸŽ¨\", variant=\"primary\")\n",
        "\n",
        "    with gr.Row():\n",
        "        source_gallery = gr.Gallery(label=\"Source Images\", columns=3, object_fit=\"contain\")\n",
        "        styled_gallery = gr.Gallery(label=\"Stylized Images\", columns=3, object_fit=\"contain\")\n",
        "\n",
        "    with gr.Row():\n",
        "        style_info = gr.Textbox(label=\"Style Analysis Details\", lines=10)\n",
        "\n",
        "    status = gr.Textbox(label=\"Processing Status\", visible=True)\n",
        "\n",
        "    def wrapper_fn(story_text):\n",
        "        try:\n",
        "            yield [[], [], \"\", \"Starting processing...\"]  # Empty galleries initially\n",
        "\n",
        "            scenes_details = process_story(story_text)\n",
        "\n",
        "            source_images = []\n",
        "            styled_images = []\n",
        "            style_details = []\n",
        "\n",
        "            for detail in scenes_details:\n",
        "                source_images.append((detail[\"source_image\"], f\"Source: {detail['scene_text']}\"))\n",
        "                styled_images.append((detail[\"styled_image\"], f\"Styled: {detail['scene_text']}\"))\n",
        "\n",
        "                # Enhanced style details with NLP analysis\n",
        "                analysis = detail.get('analysis_details', {})\n",
        "                style_details.append(\n",
        "                    f\"Scene: {detail['scene_text']}\\n\"\n",
        "                    f\"Style Applied: {detail['style_applied']}\\n\"\n",
        "                    f\"Keywords Used: {', '.join(detail['keywords_used'])}\\n\"\n",
        "                    f\"Emotion Analysis: {analysis.get('emotion_analysis', {})}\\n\"\n",
        "                    f\"Semantic Score: {analysis.get('semantic_analysis', {})}\\n\"\n",
        "                    f\"---\"\n",
        "                )\n",
        "\n",
        "            yield [source_images, styled_images, \"\\n\\n\".join(style_details), \"Processing complete!\"]\n",
        "\n",
        "        except Exception as e:\n",
        "            yield [[], [], \"\", f\"âŒ Error: {str(e)}\"]\n",
        "            raise\n",
        "\n",
        "    generate_btn.click(\n",
        "        fn=wrapper_fn,\n",
        "        inputs=story_input,\n",
        "        outputs=[source_gallery, styled_gallery, style_info, status]\n",
        "    )\n",
        "\n",
        "app.launch(server_name=\"0.0.0.0\", share=True, debug=True)\n"
      ],
      "metadata": {
        "id": "9re_2F5fCFGi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}